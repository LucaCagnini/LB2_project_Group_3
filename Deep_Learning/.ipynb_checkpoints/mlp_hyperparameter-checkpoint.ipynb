{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8407a5-2b98-482e-8e9d-69c875414880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch --quiet\n",
    "#!pip install ray --quiet\n",
    "#!pip install pydantic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f612f5a-f1ec-4254-8d21-eabac071a5f8",
   "metadata": {},
   "source": [
    "## Model definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e5a3f-6a13-45cb-8208-41c718ac983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "# Define the model\n",
    "class SP_MLP(nn.Module): #eredita da nn.module alcune caratteristiche come la capacità di memorizzare i pesi\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size, dropout_p=0.5):\n",
    "        super(SP_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) #ogni neurone in questo layer è connesso a tutti gli input. Indica quante feature riceve in input, per me questo sarà 39. e li trasformerà in 512 output.\n",
    "        self.relu1 = nn.ReLU() #è una funzione di attivazione, se un numero in input è negativo lo trasforma in 0, altrimenti lo lascia invariato. Permette di disegnare pattern piu complessi di semplici combinazioni di regressioni lineari. Il fatto di trattare i dati negativi come 0 non è una perdita di dati, poichè 0 significa nessuna ricorrenza in quei dati, non ti interessa sapere \"quanto non è presente il pattern in quel dato\".\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p) #è una tecnica di regolarizzazione e previene l'overfitting, spegnendo casualmnente una percentuale (che in questo caso è dropout_p) dei neuroni di quel layer.\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2) #rifa le stesse cose di prima, prendendo i 512 output del layer precedente e trasformandoli in 256, e poi in 32 \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=dropout_p)\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size) #prende i 32 blocchi precedenti e li comprime in uno solo (output size)\n",
    "        self.sigmoid = nn.Sigmoid() #schaiccia il valore prodotto da fc4 in un intervallo tra 0 e 1.\n",
    "\n",
    "    def forward(self, x): #dice ai dati dove andare passo dopo passo ogni qualcvolta chiami il modello. Prende un batch di dati x e lo fa passare per ogni stazione/layer\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.dropout3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define a custom dataset\n",
    "class HelixDataset(Dataset): #praticamente si tratta di tradurre i dati grezzi , ovvero le matrici numpy in formato che dataloader può capire e usare\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32) #converte la matrice x in un tensore di pytorch , ovvero la versione pytorch degli array numpy. I tensori sono oggetti particolari che possono essere spostati sulla GPU e essere usati per calcolare i gradienti.\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1) #trasforma la matrice y di forma (4809,) in (4809,1) per renderla compatibile con l'output del modello che adotta questa forma.\n",
    "    def __len__(self):\n",
    "        return len(self.X) #risponde alla domanda \"quanti campioni ci sono nel dataset?\" e usa questo numero per sapere quanti batch può creare quando un'epoca è finita (ovvero quando ha guardato tutti i dati , ovvero ha terminato un epoca). Semplicemente divide la lunghezza totale del dataset per la dimensione del batch, per calcolarsi quanti batch completi corrispondono a un epoca. In ogni modo questa funzione fornsice la lunghezza del dataset per questo calcolo.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx] #raccoglie la i-esima riga nella matrice x e y, a ogni chiamata del dataloader (quando è in modalità shuffle, e quindi li pesca a caso è molto importante, perche grazie a questa funzione lui va a pescare il numero chiamato dallo shuffle, per riga di campione e classe).\n",
    "\n",
    "\n",
    "#def train_val è la funzione che gestisce l'intero processo di addestramento e validazione. \n",
    "def train_val(model, #è il modello da addestrare\n",
    "              train_loader, #i dati da studiare, diviso in batch\n",
    "              val_loader, #il test da fare a fine  di ogni studio\n",
    "              optimizer, #metodo di studio (adam, RMSprop ecc.. dice al modlelo come aggiornare le sue consocenze).\n",
    "              criterion, #il correttore, che dice al modello di quanto ha sbagliato\n",
    "              epochs, #quante volte il modello rileggerà i dati per impararne\n",
    "              patience, #quante volte il modello può fare un esame di prova peggiore del precedente prima di interrompere le epoche in anticipo\n",
    "              scorer = matthews_corrcoef,\n",
    "              init_best_score = -1,\n",
    "              output_transform = lambda x: (x > 0.5).float()): #come tradurre la probabilità del modello, praticamente trasforma i valori in 1 e 0\n",
    "  best_val_score = init_best_score #inizializza il miglior punteggio\n",
    "  epochs_without_improvement = 0 #contatore della patience utile per vedere quante volte di fila non migliora\n",
    "  best_model_state_dict = None #prepara il cassetto dove inserirci il modello che ha performato meglio\n",
    "\n",
    "  for epoch in range(epochs): #ripeti il processo per epoche volte. \n",
    "      # Training\n",
    "      model.train()  #inizializzi il modello vuoto da allenare\n",
    "      loss = 0 #inizializzi la variabile per la loss\n",
    "      for batch_X, batch_y in train_loader: #questo for itera su tutti i batches\n",
    "          batch_X, batch_y = batch_X.to(device), batch_y.to(device) #sposta eventualmente i dati del batch sulla gpu se disponibile per fare i calcoli piu velocemente\n",
    "          optimizer.zero_grad() #azzera l'optimizer che era stato utilizzato per il batch precedente\n",
    "          outputs = model(batch_X) #il modello legge il batch x e produce le risposte\n",
    "          loss = criterion(outputs, batch_y) # il correttore calcola il singolo numero di errore confrontando le risposte date dal modello con quelle del batch y\n",
    "          loss.backward() #funzione di pytorch che  si guarda quanto ogni peso ha contribuito a quell'errore  tramite il calcolo del gradiente quindi dice di quanto un peso deve scendere o salire.\n",
    "          optimizer.step() #prende i calcoli della backward e aggiorna fisicamente i pesi del cervello per ridurre l'errore\n",
    "\n",
    "      # Validation\n",
    "      model.eval() #è cruciale perche mette il modello in fase di valutazione, spegnendo il dropout, ovvero quello che spegneva neuroni a caso per evitare overfitting\n",
    "      val_preds = []\n",
    "      val_labels = []\n",
    "      with torch.no_grad(): #dice a pytorch di non calcolare gradienti, poiche siamo in fase di valutazione, rendendo il tutto piu veloce e consumando meno memoria\n",
    "          for batch_X, batch_y in val_loader: #itera su tutti i batch del validation\n",
    "              batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "              outputs = model(batch_X)\n",
    "              #preds = (outputs > 0.5).float() #qui invece utilizzi direttamente questo modo per trasformare gli output in 0 e 1\n",
    "              preds = output_transform(outputs) #utilizza il metodo di traformazione conenuto in output _transform permettendolo di variare a piacimento\n",
    "              val_preds.extend(preds.cpu().numpy().flatten()) #aggiunge le risposte alle liste\n",
    "              val_labels.extend(batch_y.cpu().numpy().flatten())\n",
    "      val_score = scorer(val_labels, val_preds) #calcola il punteggio MCC alla fine di ogni test\n",
    "\n",
    "      if val_score > best_val_score:\n",
    "          best_val_score = val_score\n",
    "          epochs_without_improvement = 0\n",
    "          best_model_state_dict = model.state_dict()\n",
    "          print('Validation score improved to {:.4f}'.format(best_val_score))\n",
    "      else:\n",
    "          epochs_without_improvement += 1\n",
    "          if epochs_without_improvement >= patience:\n",
    "              print('Early stopping at epoch {}'.format(epoch+1))\n",
    "              break\n",
    "\n",
    "      print('Epoch [{}/{}], Loss: {:.4f}, Val score: {:.4f}'.format(epoch+1, epochs, loss.item(), val_score))\n",
    "  return best_model_state_dict\n",
    "\n",
    "def test(model, test_loader, scorer = matthews_corrcoef, output_transform = lambda x: (x > 0.5).float()):\n",
    "  model.eval()\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "      for batch_X, batch_y in test_loader:\n",
    "          batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "          outputs = model(batch_X)\n",
    "          preds = output_transform(outputs)\n",
    "          all_preds.extend(preds.cpu().numpy().flatten())\n",
    "          all_labels.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "  score = scorer(all_labels, all_preds)\n",
    "  return score\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e34a6d-2e52-46e1-8bda-e6f8feb82c1e",
   "metadata": {},
   "source": [
    "## Finding the best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefa8e7-b46f-4644-a679-8d7aeb4e61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e51c66-af93-4ecb-81cc-171467da73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the configuration on the random search must work\n",
    "config = {\n",
    "    \"base_dir\": os.path.abspath(\"../Feature_Selection/\"),\n",
    "    \"h1\": tune.choice([256, 512]), #hidden layer 1 dimension\n",
    "    \"h2\": tune.choice([128, 256]), #hidden layer 2 dimension\n",
    "    \"h3\": tune.choice([32, 64]), #hidden layer 3 dimension\n",
    "    \"dropout\": tune.uniform(0.1, 0.5), #dropout percentage\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2), #learning rate\n",
    "    \"batch_size\": tune.choice([64, 128, 256]) #batch size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa791809-c403-40a4-be80-e01b1304eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function makes the 5-CV to evaluate the mean mcc for that configuration of hyperparameter\n",
    "def test_config(config):\n",
    "    # Esempio: usa i tuoi 5 split .npz per fare MCC medio\n",
    "    mcc_scores = []\n",
    "    base_dir = config[\"base_dir\"]\n",
    "    for i in range(1, 6):\n",
    "        #this line define the folder where npz(s) files are contained and the respective path\n",
    "        train_path = os.path.join(base_dir, f\"training_features_{i}.npz\")\n",
    "        val_path   = os.path.join(base_dir, f\"validation_features_{i}.npz\")\n",
    "        test_path  = os.path.join(base_dir, f\"testing_features_{i}.npz\")\n",
    "    # load feature matrices and label vector\n",
    "        #load train\n",
    "        loaded_data_train = np.load(train_path)\n",
    "        x_train = loaded_data_train['matrix']\n",
    "        y_train = loaded_data_train['target']\n",
    "        \n",
    "        # load test\n",
    "        loaded_data_test = np.load(test_path)\n",
    "        x_test = loaded_data_test['matrix']\n",
    "        y_test = loaded_data_test['target']\n",
    "        \n",
    "        # load validation\n",
    "        loaded_data_validation = np.load(val_path)\n",
    "        x_val = loaded_data_validation['matrix']\n",
    "        y_val = loaded_data_validation['target']\n",
    "    \n",
    "            # Split the dataset into training, validation and test sets\n",
    "        train_dataset = HelixDataset(x_train, y_train)\n",
    "        val_dataset = HelixDataset(x_val, y_val)\n",
    "        test_dataset = HelixDataset(x_test, y_test)\n",
    "\n",
    "                # Create data loaders divided in batches\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize the model\n",
    "        input_size = x_train.shape[1]\n",
    "        \n",
    "        model = SP_MLP(\n",
    "            input_size,\n",
    "            config[\"h1\"], config[\"h2\"], config[\"h3\"],\n",
    "            output_size=1,\n",
    "            dropout_p=config[\"dropout\"]).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"]) #use adam optimized with learning rate chosen by random search\n",
    "        criterion = nn.BCELoss()  #loss function for binary classification\n",
    "\n",
    "        best_state = train_val(model, train_loader, val_loader,\n",
    "                               optimizer, criterion,\n",
    "                               epochs=100, patience=20)\n",
    "        model.load_state_dict(best_state) #best state(with optimized weight) with chosen parameter \n",
    "        \n",
    "        # calcoli MCC sul test di quel fold \n",
    "        mcc = test(model, test_loader)\n",
    "        mcc_scores.append(mcc)\n",
    "\n",
    "    mean_mcc = np.mean(mcc_scores)\n",
    "\n",
    "    # raytune vuole che gli riporti un dizionario di metriche\n",
    "    tune.report({\"mcc\": mean_mcc, \"loss\": -mean_mcc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74848f77-4b47-4c13-87ac-f6296c307a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric=\"mcc\", #which metric use to evaluate which trials are good or not\n",
    "    mode=\"max\", #which method to use (for example for mcc we want that it maximize, for loss indeed minimize)\n",
    "    max_t=100,   # number of max epoche\n",
    "    grace_period=1, #minimum number of epoche that the each trial must do befor be judge\n",
    "    reduction_factor=2 #halving factor, it says how much trial must be terminated at each round of ASHA. In our case it is indifferent because we have only one round of ASHA\n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    test_config,\n",
    "    config=config,\n",
    "    num_samples=20,      # how many combinations you try \n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "best_trial = result.get_best_trial(\"mcc\", \"max\", \"last\") #search all trials (combination of configurations) and select the best one. It compares the last reported mcc from each trial and returns the trial that achieved the maximum mcc.\n",
    "print(\"Best trial config:\", best_trial.config) #take the best configuration\n",
    "print(\"Best CV MCC:\", best_trial.last_result[\"mcc\"]) #take the best mcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398fc5a-4e17-4b2a-9d1b-ed5b53466292",
   "metadata": {},
   "source": [
    "## Benchmark set and final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b92bba-5d90-4222-ba22-32864cf8ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
