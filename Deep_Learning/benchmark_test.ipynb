{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7a077c-0ae0-48c0-b3b3-15bc11880b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f282d4-6bff-48be-987c-43be1677e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_sets(dataset):\n",
    "    sets = [\"1\",\"2\",\"3\",\"4\",\"5\",\"Benchmark\"]\n",
    "    set_results=[]\n",
    "    for i in sets:\n",
    "        st=dataset.query(f\"Set == '{i}'\")\n",
    "        tmp_x=[]\n",
    "        tmp_y=[]\n",
    "        for _ , row in st.iterrows():\n",
    "            seq=row[\"Sequence\"]\n",
    "            if len(seq) < 90:\n",
    "                seq=_increase_lenseq(seq)\n",
    "            else:\n",
    "                seq=seq[:90]\n",
    "            encoded_seq = one_hot_encoding(seq)\n",
    "            tmp_x.append(encoded_seq)\n",
    "            \n",
    "            if row[\"Class\"] == \"Positive\":\n",
    "                y=1\n",
    "            else:\n",
    "                y=0\n",
    "            tmp_y.append(y)\n",
    "\n",
    "        tmp_x=np.array(tmp_x, dtype=np.float32)\n",
    "        tmp_y=np.array(tmp_y, dtype=np.float32)\n",
    "        set_results.append((tmp_x , tmp_y))\n",
    "    return set_results\n",
    "            \n",
    "def _increase_lenseq(seq):\n",
    "    x=len(seq)\n",
    "    num_of_X= 90-x\n",
    "    seq=seq+(\"X\"*num_of_X)\n",
    "    return seq\n",
    "            \n",
    "def one_hot_encoding(sequence):\n",
    "    M = []\n",
    "    aa_alph = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y','X']\n",
    "    for aa in sequence:\n",
    "        one_hot = np.zeros(21)\n",
    "        try:\n",
    "            index = aa_alph.index(aa)\n",
    "            one_hot[index] = 1\n",
    "        except:\n",
    "            pass\n",
    "        M.append(one_hot)\n",
    "    M = np.array(M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060ed71c-b0eb-4a46-9f36-c9d0e17bbf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SP_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes,lstm_hidden_size, num_lstm_layers, output_size, dropout_p=0.5):\n",
    "        super(SP_NN, self).__init__()\n",
    "# 1. LSTM (Parte fissa che estrae le features temporali)\n",
    "        # input_size = 21 (One-Hot)\n",
    "        self.cnn_out_channels = 64\n",
    "        self.conv1 = nn.Conv1d(input_size, self.cnn_out_channels, kernel_size=17, padding='same')\n",
    "        self.lstm = nn.LSTM(self.cnn_out_channels, lstm_hidden_size, num_lstm_layers, \n",
    "                            batch_first=True, dropout=dropout_p if num_lstm_layers > 1 else 0)\n",
    "        \n",
    "        # 2. Batch Norm (Stabilizza l'output dell'LSTM)\n",
    "        self.bn = nn.BatchNorm1d(lstm_hidden_size)\n",
    "        \n",
    "        # 3. COSTRUZIONE DINAMICA DELL'MLP (La parte che hai chiesto)\n",
    "        mlp_layers = []\n",
    "        \n",
    "        # ATTENZIONE: L'input dell'MLP è l'output dell'LSTM!\n",
    "        current_input_size = lstm_hidden_size \n",
    "        \n",
    "        # Ciclo dinamico preso dal tuo vecchio codice\n",
    "        for hidden_size in hidden_sizes:\n",
    "            # Layer Lineare\n",
    "            mlp_layers.append(nn.Linear(current_input_size, hidden_size))\n",
    "            # Attivazione\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            # Dropout\n",
    "            mlp_layers.append(nn.Dropout(p=dropout_p))\n",
    "            # Aggiorna dimensione\n",
    "            current_input_size = hidden_size\n",
    "            \n",
    "        # Layer finale di output (riduzione a 1)\n",
    "        mlp_layers.append(nn.Linear(current_input_size, output_size))\n",
    "        \n",
    "        # Sigmoide finale (Se usi BCELoss. Se usi BCEWithLogitsLoss, toglilo!)\n",
    "        mlp_layers.append(nn.Sigmoid()) \n",
    "        \n",
    "        # Impacchetta tutto nel Sequential\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.conv1(x)       # Esce [Batch, 64, 90]\n",
    "        \n",
    "        # --- PASSAGGIO LSTM ---\n",
    "        # L'LSTM vuole [Batch, Lunghezza, Canali] -> permutiamo indietro\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :] # Prendi solo l'output finale (il \"riassunto\" dopo aver letto tutta la sequenza durante l'lstm) in pratica trasforma il vettore Batch,90,Hidden in Batch,Hidden. In sintesi fa si che l'output sia definito al 90-esimo timestamp, ovvero quando lstm ha letto tutti e 90 gli aminaocidi e ha formulato l'ipotesi con la conoscenza dei 90 precedenti aminaoccidi.\n",
    "        out = self.bn(out)\n",
    "\n",
    "        # Decode the hidden state of each time step\n",
    "        out = self.mlp(out)\n",
    "        return out\n",
    "\n",
    "# Define a custom dataset\n",
    "class SignalDataset(Dataset): #prepara i dati convertendoli in tensori\n",
    "    def __init__(self, X, y):\n",
    "        # Tieni i dati come sono (Numpy o Liste). NON convertirli subito.\n",
    "        # Questo non occupa memoria extra.\n",
    "        self.X = X \n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Converti in Tensore SOLO quando il dato viene richiesto\n",
    "        #    Questo risparmia tantissima RAM.\n",
    "        x_out = torch.from_numpy(self.X[idx]).float() \n",
    "        \n",
    "        # 2. Gestione Etichetta (Label)\n",
    "        #    .view(1) o .unsqueeze(0) serve per trasformare lo scalare \"0\" in un vettore \"[0]\"\n",
    "        #    Questo evita errori con la BCELoss che si aspetta dimensioni compatibili.\n",
    "        y_out = torch.tensor(self.y[idx], dtype=torch.float32).view(1)\n",
    "        \n",
    "        return x_out, y_out\n",
    "\n",
    "\n",
    "def train_val(model, #è il modello da addestrare\n",
    "              train_loader, #i dati da studiare, diviso in batch\n",
    "              val_loader, #il test da fare a fine  di ogni studio\n",
    "              optimizer, #metodo di studio (adam, RMSprop ecc.. dice al modlelo come aggiornare le sue consocenze).\n",
    "              criterion, #il correttore, che dice al modello di quanto ha sbagliato\n",
    "              epochs, #quante volte il modello rileggerà i dati per impararne\n",
    "              patience, #quante volte il modello può fare un esame di prova peggiore del precedente prima di interrompere le epoche in anticipo\n",
    "              scorer = matthews_corrcoef,\n",
    "              init_best_score = -1,\n",
    "              output_transform = lambda x: (x > 0.5).float()): #come tradurre la probabilità del modello, praticamente trasforma i valori in 1 e 0\n",
    "  best_val_score = init_best_score #inizializza il miglior punteggio\n",
    "  epochs_without_improvement = 0 #contatore della patience utile per vedere quante volte di fila non migliora\n",
    "  best_model_state_dict = None #prepara il cassetto dove inserirci il modello che ha performato meglio\n",
    "\n",
    "  for epoch in range(epochs): #ripeti il processo per epoche volte. \n",
    "      # Training\n",
    "      model.train()  #inizializzi il modello vuoto da allenare\n",
    "      loss = 0 #inizializzi la variabile per la loss\n",
    "      for batch_X, batch_y in train_loader: #questo for itera su tutti i batches\n",
    "          batch_X, batch_y = batch_X.to(device), batch_y.to(device) #sposta eventualmente i dati del batch sulla gpu se disponibile per fare i calcoli piu velocemente\n",
    "          optimizer.zero_grad() #azzera l'optimizer che era stato utilizzato per il batch precedente\n",
    "          outputs = model(batch_X) #il modello legge il batch x e produce le risposte\n",
    "          loss = criterion(outputs, batch_y) # il correttore calcola il singolo numero di errore confrontando le risposte date dal modello con quelle del batch y\n",
    "          loss.backward() #funzione di pytorch che  si guarda quanto ogni peso ha contribuito a quell'errore  tramite il calcolo del gradiente quindi dice di quanto un peso deve scendere o salire.\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step() #prende i calcoli della backward e aggiorna fisicamente i pesi del cervello per ridurre l'errore\n",
    "\n",
    "      # Validation\n",
    "      model.eval() #è cruciale perche mette il modello in fase di valutazione, spegnendo il dropout, ovvero quello che spegneva neuroni a caso per evitare overfitting\n",
    "      val_preds = []\n",
    "      val_labels = []\n",
    "      with torch.no_grad(): #dice a pytorch di non calcolare gradienti, poiche siamo in fase di valutazione, rendendo il tutto piu veloce e consumando meno memoria\n",
    "          for batch_X, batch_y in val_loader: #itera su tutti i batch del validation\n",
    "              batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "              outputs = model(batch_X)\n",
    "              #preds = (outputs > 0.5).float() #qui invece utilizzi direttamente questo modo per trasformare gli output in 0 e 1\n",
    "              preds = output_transform(outputs) #utilizza il metodo di traformazione conenuto in output _transform permettendolo di variare a piacimento\n",
    "              val_preds.extend(preds.cpu().numpy().flatten()) #aggiunge le risposte alle liste\n",
    "              val_labels.extend(batch_y.cpu().numpy().flatten())\n",
    "      val_score = scorer(val_labels, val_preds) #calcola il punteggio MCC alla fine di ogni test\n",
    "\n",
    "      if val_score > best_val_score:\n",
    "          best_val_score = val_score\n",
    "          epochs_without_improvement = 0\n",
    "          best_model_state_dict = model.state_dict()\n",
    "          print('Validation score improved to {:.4f}'.format(best_val_score))\n",
    "      else:\n",
    "          epochs_without_improvement += 1\n",
    "          if epochs_without_improvement >= patience:\n",
    "              print('Early stopping at epoch {}'.format(epoch+1))\n",
    "              break\n",
    "\n",
    "      print('Epoch [{}/{}], Loss: {:.4f}, Val score: {:.4f}'.format(epoch+1, epochs, loss.item(), val_score))\n",
    "  return best_model_state_dict\n",
    "\n",
    "def test(model, test_loader, scorer = matthews_corrcoef, output_transform = lambda x: (x > 0.5).float()):\n",
    "  model.eval()\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "      for batch_X, batch_y in test_loader:\n",
    "          batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "          outputs = model(batch_X)\n",
    "          preds = output_transform(outputs)\n",
    "          all_preds.extend(preds.cpu().numpy().flatten())\n",
    "          all_labels.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "  score = scorer(all_labels, all_preds)\n",
    "  return score\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc14b61-157c-41cc-9d4a-987af9e71a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {'num_layers': 4, 'hidden_sizes': [256, 128, 64, 1024], 'dropout': 0.4980673167779849, 'lr': 0.00028585527498522286, 'batch_size': 20, 'num_lstm_layers': 2, 'lstm_hidden_size': 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899adc4c-545f-49cd-b037-98c5ebc2b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carica tutto il CSV\n",
    "dataset = pd.read_csv(\"../Data_Preparation/train_bench.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 2. Elabora tutto in una volta (La lista conterrà 6 elementi ordinati)\n",
    "all_data = create_one_hot_sets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573157e7-298d-4f62-87a5-bb2f24cf04f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score improved to 0.0000\n",
      "Epoch [1/100], Loss: 0.5829, Val score: 0.0000\n",
      "Epoch [2/100], Loss: 0.1066, Val score: 0.0000\n",
      "Epoch [3/100], Loss: 0.2617, Val score: 0.0000\n",
      "Epoch [4/100], Loss: 0.2689, Val score: 0.0000\n",
      "Epoch [5/100], Loss: 0.1968, Val score: 0.0000\n",
      "Epoch [6/100], Loss: 0.4944, Val score: 0.0000\n",
      "Epoch [7/100], Loss: 0.1981, Val score: 0.0000\n",
      "Epoch [8/100], Loss: 0.3146, Val score: 0.0000\n",
      "Epoch [9/100], Loss: 0.2615, Val score: 0.0000\n",
      "Epoch [10/100], Loss: 0.1170, Val score: 0.0000\n",
      "Epoch [11/100], Loss: 0.1376, Val score: 0.0000\n",
      "Validation score improved to 0.7968\n",
      "Epoch [12/100], Loss: 0.1510, Val score: 0.7968\n",
      "Epoch [13/100], Loss: 0.0895, Val score: 0.7873\n",
      "Validation score improved to 0.8740\n",
      "Epoch [14/100], Loss: 0.0393, Val score: 0.8740\n",
      "Epoch [15/100], Loss: 0.0566, Val score: 0.8670\n",
      "Epoch [16/100], Loss: 0.0476, Val score: 0.8596\n",
      "Epoch [17/100], Loss: 0.0273, Val score: 0.8133\n",
      "Epoch [18/100], Loss: 0.1226, Val score: 0.7029\n",
      "Validation score improved to 0.9024\n",
      "Epoch [19/100], Loss: 0.0135, Val score: 0.9024\n",
      "Epoch [20/100], Loss: 0.0467, Val score: 0.8908\n",
      "Epoch [21/100], Loss: 0.1699, Val score: 0.8689\n",
      "Epoch [22/100], Loss: 0.0011, Val score: 0.8924\n",
      "Epoch [23/100], Loss: 0.0804, Val score: 0.8073\n",
      "Epoch [24/100], Loss: 0.2962, Val score: 0.8695\n",
      "Validation score improved to 0.9035\n",
      "Epoch [25/100], Loss: 0.0351, Val score: 0.9035\n",
      "Epoch [26/100], Loss: 0.0250, Val score: 0.8915\n",
      "Epoch [27/100], Loss: 0.0036, Val score: 0.8678\n",
      "Epoch [28/100], Loss: 0.0280, Val score: 0.8312\n",
      "Epoch [29/100], Loss: 0.0022, Val score: 0.8367\n",
      "Epoch [30/100], Loss: 0.1120, Val score: 0.8343\n",
      "Epoch [31/100], Loss: 0.0391, Val score: 0.8702\n",
      "Epoch [32/100], Loss: 0.0303, Val score: 0.8017\n",
      "Epoch [33/100], Loss: 0.1424, Val score: 0.8894\n",
      "Epoch [34/100], Loss: 0.1949, Val score: 0.8439\n",
      "Validation score improved to 0.9099\n",
      "Epoch [35/100], Loss: 0.2117, Val score: 0.9099\n",
      "Epoch [36/100], Loss: 0.0324, Val score: 0.8795\n",
      "Validation score improved to 0.9149\n",
      "Epoch [37/100], Loss: 0.0016, Val score: 0.9149\n",
      "Epoch [38/100], Loss: 0.0146, Val score: 0.8959\n",
      "Epoch [39/100], Loss: 0.1749, Val score: 0.8941\n",
      "Epoch [40/100], Loss: 0.0063, Val score: 0.9133\n",
      "Epoch [41/100], Loss: 0.0116, Val score: 0.8949\n",
      "Validation score improved to 0.9212\n",
      "Epoch [42/100], Loss: 0.0094, Val score: 0.9212\n",
      "Validation score improved to 0.9250\n",
      "Epoch [43/100], Loss: 0.0440, Val score: 0.9250\n",
      "Validation score improved to 0.9313\n",
      "Epoch [44/100], Loss: 0.0006, Val score: 0.9313\n",
      "Epoch [45/100], Loss: 0.0044, Val score: 0.9313\n",
      "Epoch [46/100], Loss: 0.0038, Val score: 0.9209\n",
      "Validation score improved to 0.9458\n",
      "Epoch [47/100], Loss: 0.0047, Val score: 0.9458\n",
      "Epoch [48/100], Loss: 0.0098, Val score: 0.9326\n",
      "Validation score improved to 0.9553\n",
      "Epoch [49/100], Loss: 0.0065, Val score: 0.9553\n",
      "Epoch [50/100], Loss: 0.0001, Val score: 0.9346\n",
      "Epoch [51/100], Loss: 0.0117, Val score: 0.9270\n",
      "Epoch [52/100], Loss: 0.0034, Val score: 0.9350\n",
      "Epoch [53/100], Loss: 0.0309, Val score: 0.9434\n",
      "Epoch [54/100], Loss: 0.0047, Val score: 0.9298\n",
      "Epoch [55/100], Loss: 2.5328, Val score: 0.9332\n",
      "Epoch [56/100], Loss: 0.0107, Val score: 0.9458\n",
      "Epoch [57/100], Loss: 0.0048, Val score: 0.9335\n",
      "Epoch [58/100], Loss: 0.0015, Val score: 0.9197\n",
      "Epoch [59/100], Loss: 0.0009, Val score: 0.9190\n",
      "Epoch [60/100], Loss: 0.0000, Val score: 0.9309\n",
      "Epoch [61/100], Loss: 0.0002, Val score: 0.9383\n",
      "Validation score improved to 0.9596\n",
      "Epoch [62/100], Loss: 0.0003, Val score: 0.9596\n",
      "Epoch [63/100], Loss: 0.0008, Val score: 0.9425\n",
      "Epoch [64/100], Loss: 0.2548, Val score: 0.9527\n",
      "Epoch [65/100], Loss: 0.0000, Val score: 0.9313\n",
      "Epoch [66/100], Loss: 0.0000, Val score: 0.9226\n",
      "Epoch [67/100], Loss: 0.0000, Val score: 0.9556\n",
      "Epoch [68/100], Loss: 0.0021, Val score: 0.9467\n",
      "Epoch [69/100], Loss: 0.0000, Val score: 0.9408\n",
      "Epoch [70/100], Loss: 0.0000, Val score: 0.9458\n",
      "Epoch [71/100], Loss: 0.0000, Val score: 0.9490\n",
      "Epoch [72/100], Loss: 0.0018, Val score: 0.9288\n",
      "Epoch [73/100], Loss: 0.0000, Val score: 0.9350\n",
      "Epoch [74/100], Loss: 0.0001, Val score: 0.9305\n",
      "Epoch [75/100], Loss: 0.0000, Val score: 0.9492\n",
      "Epoch [76/100], Loss: 0.0013, Val score: 0.9332\n",
      "Epoch [77/100], Loss: 0.0009, Val score: 0.9324\n",
      "Epoch [78/100], Loss: 0.0001, Val score: 0.9464\n",
      "Epoch [79/100], Loss: 0.0000, Val score: 0.9359\n",
      "Epoch [80/100], Loss: 0.0000, Val score: 0.9525\n",
      "Epoch [81/100], Loss: 0.0030, Val score: 0.9445\n",
      "Early stopping at epoch 82\n",
      "MCC on benchmark set: 0.9018711109052984\n"
     ]
    }
   ],
   "source": [
    "# 2. PREPARAZIONE TRAINING (Concatenare 3 set)\n",
    "training_indices=[0,1,2,3]\n",
    "validation_index=4\n",
    "testing_index=5\n",
    "# Raccogliamo le X dei 3 set di training\n",
    "train_x_list = [all_data[j][0] for j in training_indices]\n",
    "# Raccogliamo le y dei 3 set di training\n",
    "train_y_list = [all_data[j][1] for j in training_indices]\n",
    "\n",
    "# Uniamo tutto in un unico array gigante\n",
    "x_train_conc = np.concatenate(train_x_list, axis=0)\n",
    "y_train_conc = np.concatenate(train_y_list, axis=0)\n",
    "\n",
    "# 3. PREPARAZIONE VALIDATION & TEST (Singoli set)\n",
    "x_val = all_data[validation_index][0]\n",
    "y_val = all_data[validation_index][1]\n",
    "\n",
    "x_test = all_data[testing_index][0]\n",
    "y_test = all_data[testing_index][1]\n",
    "\n",
    "# 4. CREAZIONE DATASET (Usa la tua classe SignalDataset lazy)\n",
    "# Nota: Non serve trasformare in tensori qui, lo fa il Dataset dentro __getitem__\n",
    "train_dataset = SignalDataset(x_train_conc, y_train_conc)\n",
    "val_dataset = SignalDataset(x_val, y_val)\n",
    "test_dataset = SignalDataset(x_test, y_test)\n",
    "\n",
    "# 5. DATALOADERS\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])\n",
    "# --- MODELLO ---\n",
    "model = SP_NN(\n",
    "input_size=21, \n",
    "hidden_sizes=config[\"hidden_sizes\"],\n",
    "lstm_hidden_size=config[\"lstm_hidden_size\"],\n",
    "num_lstm_layers=config[\"num_lstm_layers\"],\n",
    "output_size=1,\n",
    "dropout_p=config[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training (Verbose False per pulizia)\n",
    "best_state = train_val(model, train_loader, val_loader, optimizer, criterion,\n",
    "                   epochs=100, patience=20)\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "mcc = test(model, test_loader)\n",
    "print(\"MCC on benchmark set:\", mcc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
